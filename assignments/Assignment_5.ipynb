{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPUT 655 - Assignment 5\n",
    "## Lucas Cruz\n",
    "\n",
    "Kept it simple this time, hope it helps to mark it.\n",
    "\n",
    "The plots and discussions were included in the PDF submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "env = gymnasium.make(\"Gym-Gridworlds/Penalty-3x3-v0\")\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "R = np.zeros((n_states, n_actions))\n",
    "P = np.zeros((n_states, n_actions, n_states))\n",
    "T = np.zeros((n_states, n_actions))\n",
    "\n",
    "env.reset()\n",
    "for s in range(n_states):\n",
    "    for a in range(n_actions):\n",
    "        env.unwrapped.set_state(s)\n",
    "        s_next, r, terminated, _, _ = env.step(a)\n",
    "        R[s, a] = r\n",
    "        P[s, a, s_next] = 1.0\n",
    "        T[s, a] = terminated\n",
    "\n",
    "P = P * (1.0 - T[..., None])  # next state probability for terminal transitions is 0\n",
    "\n",
    "def bellman_q(pi, gamma, max_iter=1000):\n",
    "    delta = np.inf\n",
    "    iter = 0\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    be = np.zeros((max_iter))\n",
    "    while delta > 1e-5 and iter < max_iter:\n",
    "        Q_new = R + (np.dot(P, gamma * (Q * pi)).sum(-1))\n",
    "        delta = np.abs(Q_new - Q).sum()\n",
    "        be[iter] = delta\n",
    "        Q = Q_new\n",
    "        iter += 1\n",
    "    return Q\n",
    "\n",
    "def eps_greedy_action(Q, s, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.choice(n_actions)  # Explore\n",
    "    else:\n",
    "        max_value = Q[s].max()\n",
    "        ties = [i for i, v in enumerate(Q[s]) if v == max_value]\n",
    "        return np.random.choice(ties) # Exploit - random tie\n",
    "\n",
    "def expected_return(env, Q, gamma, episodes=10):\n",
    "    G = np.zeros(episodes)\n",
    "    for e in range(episodes):\n",
    "        s, _ = env.reset(seed=e)\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done:\n",
    "            a = eps_greedy_action(Q, s, 0.0)\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            G[e] += gamma**t * r\n",
    "            s = s_next\n",
    "            t += 1\n",
    "    return G.mean()\n",
    "\n",
    "def td(env, env_eval, Q, gamma, eps, alpha, max_steps, alg):\n",
    "    be = []\n",
    "    exp_ret = []\n",
    "    tde = np.zeros(max_steps)\n",
    "    eps_decay = eps / max_steps\n",
    "    alpha_decay = alpha / max_steps\n",
    "    tot_steps = 0\n",
    "\n",
    "    while tot_steps < max_steps:\n",
    "        s_t, info = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        if alg == \"QL\":\n",
    "            # play one episode\n",
    "            while (not done): # the environment handles max horizon\n",
    "                a_t = eps_greedy_action(Q, s_t, eps)\n",
    "                s_prime, reward, terminated, truncated, info = env.step(a_t)\n",
    "                if tot_steps >= max_steps-1: # if we reach the total steps, truncate it\n",
    "                    truncated = True\n",
    "\n",
    "                # Compute the future Q value\n",
    "                future_q_value = Q[s_prime].max()\n",
    "                temporal_difference = reward + gamma * future_q_value - Q[s_t][a_t]\n",
    "\n",
    "                # Update Q-values\n",
    "                Q[s_t][a_t] += alpha * temporal_difference\n",
    "                tde[tot_steps] = np.abs(temporal_difference)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                s_t = s_prime\n",
    "                tot_steps += 1\n",
    "                eps = max(eps - eps_decay, 0.01)\n",
    "                alpha = max(alpha - alpha_decay, 0.001)\n",
    "\n",
    "                a_star = [eps_greedy_action(Q, s_i, eps=eps) for s_i in range(n_states)]\n",
    "                pi_greedy = np.eye(n_states, n_actions)[a_star]\n",
    "                if tot_steps % 100 == 0:\n",
    "                    be.append(np.mean(np.abs(bellman_q(pi_greedy, gamma) - Q)))\n",
    "                    exp_ret.append(expected_return(env_eval, Q, gamma, episodes=10))\n",
    "\n",
    "        elif alg == \"SARSA\":\n",
    "            a_t = eps_greedy_action(Q, s_t, eps)\n",
    "\n",
    "            # play one episode\n",
    "            while (not done): # the environment handles max horizon                \n",
    "                s_prime, reward, terminated, truncated, info = env.step(a_t)\n",
    "                if tot_steps >= max_steps-1: # if we reach the total steps, truncate it\n",
    "                    truncated = True\n",
    "\n",
    "                # Compute the future Q value\n",
    "                a_prime = eps_greedy_action(Q, s_prime, eps)\n",
    "                future_q_value = Q[s_prime, a_prime]\n",
    "                temporal_difference = reward + gamma * future_q_value - Q[s_t][a_t]\n",
    "\n",
    "                # Update Q-values\n",
    "                Q[s_t][a_t] += alpha * temporal_difference\n",
    "                tde[tot_steps] = np.abs(temporal_difference)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                s_t = s_prime\n",
    "                a_t = a_prime\n",
    "                tot_steps += 1\n",
    "                eps = max(eps - eps_decay, 0.01)\n",
    "                alpha = max(alpha - alpha_decay, 0.001)\n",
    "\n",
    "                a_star = [eps_greedy_action(Q, s_i, eps=eps) for s_i in range(n_states)]\n",
    "                pi_greedy = np.eye(n_states, n_actions)[a_star]\n",
    "                if tot_steps % 100 == 0:\n",
    "                    be.append(np.mean(np.abs(bellman_q(pi_greedy, gamma) - Q)))\n",
    "                    exp_ret.append(expected_return(env_eval, Q, gamma, episodes=10))\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    return Q, be, tde, exp_ret\n",
    "\n",
    "# https://stackoverflow.com/a/63458548/754136\n",
    "def smooth(arr, span):\n",
    "    re = np.convolve(arr, np.ones(span * 2 + 1) / (span * 2 + 1), mode=\"same\")\n",
    "    re[0] = arr[0]\n",
    "    for i in range(1, span + 1):\n",
    "        re[i] = np.average(arr[: i + span])\n",
    "        re[-i] = np.average(arr[-i - span :])\n",
    "    return re\n",
    "\n",
    "def error_shade_plot(ax, data, stepsize, smoothing_window=1, **kwargs):\n",
    "    y = np.nanmean(data, 0)\n",
    "    x = np.arange(len(y))\n",
    "    x = [stepsize * step for step in range(len(y))]\n",
    "    if smoothing_window > 1:\n",
    "        y = smooth(y, smoothing_window)\n",
    "    (line,) = ax.plot(x, y, **kwargs)\n",
    "    error = np.nanstd(data, axis=0)\n",
    "    if smoothing_window > 1:\n",
    "        error = smooth(error, smoothing_window)\n",
    "    error = 1.96 * error / np.sqrt(data.shape[0])\n",
    "    ax.fill_between(x, y - error, y + error, alpha=0.2, linewidth=0.0, color=line.get_color())\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "eps = 1.0\n",
    "max_steps = 10000\n",
    "horizon = 10\n",
    "\n",
    "init_values = [-10, 0.0, 10]\n",
    "# algs = [\"QL\", \"SARSA\", \"Exp_SARSA\"]\n",
    "algs = [\"QL\", \"SARSA\"]\n",
    "# seeds = np.arange(50)\n",
    "seeds = np.arange(5)\n",
    "\n",
    "results_be = np.zeros((\n",
    "    len(init_values),\n",
    "    len(algs),\n",
    "    len(seeds),\n",
    "    max_steps // 100,\n",
    "))\n",
    "results_tde = np.zeros((\n",
    "    len(init_values),\n",
    "    len(algs),\n",
    "    len(seeds),\n",
    "    max_steps,\n",
    "))\n",
    "results_exp_ret = np.zeros((\n",
    "    len(init_values),\n",
    "    len(algs),\n",
    "    len(seeds),\n",
    "    max_steps // 100,\n",
    "))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "# plt.ion()\n",
    "# plt.show()\n",
    "\n",
    "reward_noise_std = 0.0  # re-run with 3.0\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_prop_cycle(\n",
    "        color=[\"red\", \"green\", \"blue\", \"black\", \"orange\", \"cyan\", \"brown\", \"gray\", \"pink\"]\n",
    "    )\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "\n",
    "env = gymnasium.make(\n",
    "    \"Gym-Gridworlds/Penalty-3x3-v0\",\n",
    "    max_episode_steps=horizon,\n",
    "    reward_noise_std=reward_noise_std,\n",
    ")\n",
    "\n",
    "env_eval = gymnasium.make(\n",
    "    \"Gym-Gridworlds/Penalty-3x3-v0\",\n",
    "    max_episode_steps=horizon,\n",
    ")\n",
    "\n",
    "for i, init_value in enumerate(init_values):\n",
    "    for j, alg in enumerate(algs):\n",
    "        for seed in tqdm(seeds):\n",
    "            np.random.seed(seed)\n",
    "            Q = np.zeros((n_states, n_actions)) + init_value\n",
    "            Q, be, tde, exp_ret = td(env, env_eval, Q, gamma, eps, alpha, max_steps, alg)\n",
    "            results_be[i, j, seed] = be\n",
    "            results_tde[i, j, seed] = tde\n",
    "            results_exp_ret[i, j, seed] = exp_ret\n",
    "            # print(i, j, seed)\n",
    "        label = f\"$Q_0$: {init_value}, Alg: {alg}\"\n",
    "        axs[0].set_title(\"TD Error\")\n",
    "        error_shade_plot(\n",
    "            axs[0],\n",
    "            results_tde[i, j],\n",
    "            stepsize=1,\n",
    "            smoothing_window=20,\n",
    "            label=label,\n",
    "        )\n",
    "        axs[0].legend()\n",
    "        axs[0].set_ylim([0, 5])\n",
    "        axs[1].set_title(\"Bellman Error\")\n",
    "        error_shade_plot(\n",
    "            axs[1],\n",
    "            results_be[i, j],\n",
    "            stepsize=100,\n",
    "            smoothing_window=20,\n",
    "            label=label,\n",
    "        )\n",
    "        axs[1].legend()\n",
    "        axs[1].set_ylim([0, 50])\n",
    "        axs[2].set_title(\"Expected Return\")\n",
    "        error_shade_plot(\n",
    "            axs[2],\n",
    "            results_exp_ret[i, j],\n",
    "            stepsize=100,\n",
    "            smoothing_window=20,\n",
    "            label=label,\n",
    "        )\n",
    "        axs[2].legend()\n",
    "        axs[2].set_ylim([-5, 1])\n",
    "#         plt.draw()\n",
    "#         plt.pause(0.001)\n",
    "\n",
    "# plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
